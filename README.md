# web_scraping
Module 11 Challenge


## Instructions
You’re now ready to take on a full web-scraping and data analysis project. You’ve learned to identify HTML elements on a page, identify their id and class attributes, and use this knowledge to extract information via both automated browsing with Splinter and HTML parsing with Beautiful Soup. You’ve also learned to scrape various types of information. These include HTML tables and recurring elements, like multiple news articles on a webpage.

As you work on this Challenge, remember that you’re strengthening the same core skills that you’ve been developing until now: collecting data, organizing, and storing data, analyzing data, and then visually communicating your insights.

## Objective 

The objective of this challenge was to become more familiar with HTML tags and structure of a webpage for fast and effecient data scraping wile using Splinter for web navagation.  

## Summary

###Mars News 
I used a Mars news website. I used Splinter for web navigation and BeautifulSoup for parsing HTML to extract news titles and preview texts. I identified the correct HTML elements for scraping and stored the results in a Python list of dictionaries.

###Mars Weather
Again, I used Splinter for web navigation and BeautifulSoup for parsing HTML to extract data. 

Temperature analysis: I identified the average minimum temperature for each Martian month, and the coldest and hottest months based on this data.

Pressure Analysis: I identified the average atmospheric pressure for each Martian month. 

Visualizations: Through various bar charts, I generated visualizatinos for the temperature and pressure data.

CSV export: I exported all my data into a csv file for later use. 
	

## Tools
ChatGPT: Served as an interactive AI assistant, providing real-time guidance, code troubleshooting, and explanations throughout the process.

Splinter: A Python library used for browser automation. It was utilized to programmatically navigate to the Mars news website.

BeautifulSoup: A Python library for parsing HTML and XML documents. It was critical in extracting specific elements from the webpage's HTML.

json: A Python library for working with JSON data. Used for formatting the output in a readable manner.

Jupyter Notebook: This was the environment where I run my Python scripts.

## Contributors
I worked on this challenge all by myself, with no help from any classmates, TA's, professors, or tutors! Tadaa!



